{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "The Boston dataset is a small set composed of 506 samples and 13 features used for regression problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'],random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the pipeline\n",
    "The pipeline we are going to setup is composed of the following tasks:\n",
    "\n",
    "- Data Normalization\n",
    "- Dimensionality Reduction: Principal Component Analysis (PCA) and a univariate feature selection algorithm as possible candidates.\n",
    "- Regression\n",
    "\n",
    "Start by manually implementing a pipeline without any dedicated scikit-learn module, to highlight how many repetitive activities are necessary. We are going to manually instantiate and initialize a single method for every step of the pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without a Pipeline\n",
    "(not efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "ridge = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "ridge.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is just a list of ordered elements, each with a name and a corresponding object instance. The pipeline module leverages on the common interface that every scikit-learn library must implement, such as: fit, transform and predict.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reduce_dim', PCA()),\n",
    "        ('regressor', Ridge())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get all the properties of each element in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.10913081e-17,  4.68695736e-17,  3.28087015e-17,  1.75760901e-17,\n",
       "        5.62434883e-17, -3.74956589e-17,  6.32739244e-17,  2.46065261e-17,\n",
       "       -6.32739244e-17,  2.22630475e-17, -1.05456541e-17,  1.87478294e-17,\n",
       "        5.50717490e-17])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps[0][1].mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score:  -4035.6039307016554\n"
     ]
    }
   ],
   "source": [
    "print('Testing score: ', pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to index the pipeline to access a specific element and retrieve a single value,\n",
    "for example the explained variance in the PCA step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0026455 1.0026455 1.0026455 1.0026455 1.0026455 1.0026455 1.0026455\n",
      " 1.0026455 1.0026455 1.0026455 1.0026455 1.0026455 1.0026455]\n"
     ]
    }
   ],
   "source": [
    "print(pipe.steps[1][1].explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameters are parameters that are manually tuned by a human operator to maximize the model performance against a validation set through a grid search.\n",
    "Let's start with a trivial example, where we aim at optimizing \n",
    "- the number of components selected by the PCA\n",
    "- the regularization factor of the linear regression model. \n",
    "\n",
    "We are going to use the `GridSearchCV` module in sklearn.\n",
    "\n",
    "Concerning PCA, we want to evaluate how accuracy varies with the number of components, from 1 to 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_to_test = np.arange(1, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the regularization factor, we consider an exponential range of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_to_test = 2.0**np.arange(-6, +6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to evaluate all possible combinations of the parameters and 2 possible Scalers:\n",
    "First of all, we define a dictionary with all the parameters we would like to combine in the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'reduce_dim__n_components': n_features_to_test,\n",
    "          'regressor__alpha': alpha_to_test,\n",
    "         'scaler' : [StandardScaler(), RobustScaler()]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth remarking the convention adopted to name the parameters: \n",
    "<ol>\n",
    "<li> name of the pipeline step\n",
    "<li> followed by a double underscore (__)\n",
    "<li> finally the name of the parameter within the step. \n",
    "</ol>\n",
    "The optimization is invoked as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
      "Final score is:  -33516.40161736743\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "gridsearch = GridSearchCV(pipe, params, verbose=1).fit(X_train, y_train)\n",
    "print('Final score is: ', gridsearch.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reduce_dim__n_components': 10,\n",
       " 'regressor__alpha': 8.0,\n",
       " 'scaler': StandardScaler()}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Tuning (advanced)\n",
    "In theory, we could also apply the same approach to the dimensionality reduction step, for example to choose between `PCA` and `SelectKBest`. The only problem in this case is that PCA relies on a parameter named `n_components`, while SelectKBest requires to optimize a parameter named `k`.\n",
    "\n",
    "Luckily, GridSearchCV also allows to optimize lists of parameter dictionaries, which solves this issue as well:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers_to_test=[StandardScaler(), RobustScaler()]\n",
    "params = [\n",
    "        {'scaler': scalers_to_test,\n",
    "         'reduce_dim': [PCA()],\n",
    "         'reduce_dim__n_components': n_features_to_test,\\\n",
    "         'regressor__alpha': alpha_to_test},\n",
    "\n",
    "        {'scaler': scalers_to_test,\n",
    "         'reduce_dim': [SelectKBest(f_regression)],\n",
    "         'reduce_dim__k': n_features_to_test,\\\n",
    "         'regressor__alpha': alpha_to_test}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n",
      "Final score is:  -3504.192468048363\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "gridsearch = GridSearchCV(pipe, params, verbose=1).fit(X_train, y_train)\n",
    "print('Final score is: ', gridsearch.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reduce_dim': SelectKBest(score_func=<function f_regression at 0x1254f30d0>),\n",
       " 'reduce_dim__k': 10,\n",
       " 'regressor__alpha': 16.0,\n",
       " 'scaler': StandardScaler()}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
